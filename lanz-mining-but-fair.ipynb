{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31ae0c-ab1f-4f63-be05-2fdcc94fca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "29faa9ce-1d06-4e1b-801f-9245eca1723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GastEintrag:\n",
    "    namen: List[str]      \n",
    "    beschreibung: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "6822874b-1cf0-4781-966f-6ba42a91f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_list_from_pdf(pdf_path): \n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    episodes = []\n",
    "    current_episode_parts = []\n",
    "\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            matches = re.findall(r\"Seite\\s+(\\d+)\\s+von\\s+(\\d+)\", text)\n",
    "\n",
    "            if matches:\n",
    "                # Wir nehmen das letzte Vorkommen auf der Seite, da es sich meist um die Fußzeile handelt\n",
    "                current_page_num, total_pages = map(int, matches[-1])\n",
    "\n",
    "                if current_page_num == 1:\n",
    "                        # Neue Episode beginnt\n",
    "                        current_episode_parts = [text]\n",
    "                else:\n",
    "                    # Fortsetzung einer Episode\n",
    "                    current_episode_parts.append(text)\n",
    "\n",
    "                    \n",
    "                if current_page_num == total_pages:\n",
    "                    # Text zusammenfügen und zur Ergebnisliste hinzufügen\n",
    "                    full_text = \"\\n\".join(current_episode_parts)\n",
    "                    episodes.append(full_text)\n",
    "                    current_episode_parts = [] # Puffer zurücksetzen\n",
    "\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "362a3301-bf6d-4bbb-88bf-73f352318a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_files(episodes, path):\n",
    "    \"\"\"\n",
    "    Speichert die Liste der Episoden in eine einzelne Textdatei.\n",
    "    \n",
    "    Args:\n",
    "        episodes (list): Die Liste mit den Episodentexten.\n",
    "        filename (str): Der Name der Zieldatei.\n",
    "    \"\"\"\n",
    "    # 'utf-8' ist wichtig für Umlaute (ä, ö, ü) und Sonderzeichen\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, episode in enumerate(episodes, 1):\n",
    "            # Eine Überschrift pro Episode schreiben\n",
    "            f.write(f\"--- EPISODE {i} ---\\n\\n\")\n",
    "            \n",
    "            # Den eigentlichen Text schreiben\n",
    "            f.write(episode)\n",
    "            \n",
    "            # Trennlinie für bessere Lesbarkeit am Ende jeder Episode\n",
    "            f.write(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Erfolgreich gespeichert in: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "aa49db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_episodes(episodes):\n",
    "    cleaned_episodes = []\n",
    "\n",
    "    # Einfacher Schalter: Haben wir die Folge schon gesehen?\n",
    "    hatte_15_12_schon = False\n",
    "    hatte_12_11_schon = False \n",
    "\n",
    "    scheller_pattern = r\"Scheller,.*?ZDF P r o g r a m m - D a t e n b a n k\"\n",
    "    page_pattern = r\"TV Seite\\s+\\d+\\s+von\\s+\\d+\"\n",
    "\n",
    "    for text in episodes:\n",
    "        \n",
    "        # --- SPEZIAL-CHECK FÜR Duplikate ---\n",
    "        if \"Markus Lanz 15.12.2015\" in text:\n",
    "            if hatte_15_12_schon:\n",
    "                # Wenn wir sie schon einmal hatten (True), ist das hier das Duplikat -> Rauslöschen (continue)\n",
    "                print(\"Found Duplicate: Markus Lanz 15.12.2015\")\n",
    "                continue\n",
    "            else:\n",
    "                # Das erste Mal gefunden -> Schalter umlegen und behalten\n",
    "                hatte_15_12_schon = True\n",
    "\n",
    "\n",
    "        if \"Markus Lanz 12.11.2015\" in text: \n",
    "            if hatte_12_11_schon:\n",
    "                print(\"Found Duplicate: Markus Lanz 12.11.2015\")\n",
    "                # Wenn wir sie schon einmal hatten (True), ist das hier das Duplikat -> Rauslöschen (continue)\n",
    "                continue\n",
    "            else:\n",
    "                # Das erste Mal gefunden -> Schalter umlegen und behalten\n",
    "                hatte_12_11_schon = True\n",
    "\n",
    "\n",
    "        # 1. Den Scheller-Block entfernen\n",
    "        text = re.sub(scheller_pattern, \"\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        # 2. Die Seitenzahlen entfernen\n",
    "        text = re.sub(page_pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # 3. Kosmetik\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        \n",
    "        cleaned_episodes.append(text.strip())\n",
    "        \n",
    "    return cleaned_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5cca003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrahiere_sendungs_details(text: str) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extrahiert Datum, Sendezeit-Range und Dauer aus dem rohen Textblock.\n",
    "    \"\"\"\n",
    "    daten = {\n",
    "        \"Titel\": None,\n",
    "        \"Datum\": None,\n",
    "        \"Sendezeit\": None,\n",
    "        \"Dauer\": None,\n",
    "        \"FolgenNr\": None,\n",
    "        \"Staffel\": None, \n",
    "        \"Folge\": None\n",
    "    }\n",
    "\n",
    "    match_folgen_nr = re.search(r\"FolgenNr\\s+(\\d+)\", text)\n",
    "    if match_folgen_nr:\n",
    "        daten[\"FolgenNr\"] = match_folgen_nr.group(1)\n",
    "    else: \n",
    "        daten[\"FolgenNr\"] = None\n",
    "\n",
    "    match_staffel = re.search(r\"Staffel\\s+(\\d+)\", text)\n",
    "    if match_staffel:\n",
    "        daten[\"Staffel\"] = match_staffel.group(1)\n",
    "    else:\n",
    "        daten[\"Staffel\"] = None\n",
    "\n",
    "    match_folge = re.search(r\"Folge\\s+(\\d+)\", text)\n",
    "    if match_folge:\n",
    "        daten[\"Folge\"] = match_folge.group(1)\n",
    "    else:\n",
    "        daten[\"Folge\"] = None\n",
    "\n",
    "\n",
    "    # 1. Datum suchen (Nach dem Keyword \"Publikation\")\n",
    "    if \"Publikation\" in text:\n",
    "        # Wir nehmen alles nach \"Publikation\"\n",
    "        part_publikation = text.split(\"Publikation\", 1)[1]\n",
    "        \n",
    "        # Regex für Datum (DD.MM.YYYY)\n",
    "        # Wir suchen das ERSTE Datum, das nach Publikation auftaucht\n",
    "        date_match = re.search(r\"(\\d{2}\\.\\d{2}\\.\\d{4})\", part_publikation)\n",
    "        if date_match:\n",
    "            daten[\"Datum\"] = date_match.group(1)\n",
    "\n",
    "    # 2. Sendezeit und Dauer suchen (Nach dem Keyword \"Sachinhalt\")\n",
    "    # Das Format ist dort meist: \"Sachinhalt HH:MM:SS - HH:MM:SS MMM'SS\"\n",
    "    if \"Sachinhalt\" in text:\n",
    "        part_sachinhalt = text.split(\"Sachinhalt\", 1)[1]\n",
    "        \n",
    "        # Regex Erklärung:\n",
    "        # (\\d{2}:\\d{2}:\\d{2}\\s*-\\s*\\d{2}:\\d{2}:\\d{2}) -> Gruppe 1: Zeit von - bis\n",
    "        # \\s+                                         -> Leerzeichen\n",
    "        # (\\d{2,3}'\\d{2})                             -> Gruppe 2: Dauer (z.B. 67'41 oder 067'41)\n",
    "        \n",
    "        zeit_pattern = r\"(\\d{2}:\\d{2}:\\d{2}\\s*-\\s*\\d{2}:\\d{2}:\\d{2})\\s+(\\d{2,3}'\\d{2})\"\n",
    "        \n",
    "        zeit_match = re.search(zeit_pattern, part_sachinhalt)\n",
    "        if zeit_match:\n",
    "            daten[\"Sendezeit\"] = zeit_match.group(1)\n",
    "            daten[\"Dauer\"] = zeit_match.group(2)\n",
    "\n",
    "    #Titel extrahieren\n",
    "    keyword = 'Sendetitel'\n",
    "\n",
    "    if keyword not in text:\n",
    "        daten[\"titel\"] = \"Unbekannt (Kein 'Sendetitel' gefunden)\"\n",
    "\n",
    "    parts = text.split(keyword, 1)\n",
    "\n",
    "    # 3. Prüfen, ob der Split erfolgreich war (Liste muss mind. 2 Teile haben)\n",
    "    if len(parts) < 2:\n",
    "        daten[\"titel\"] =  \"Unbekannt (Split fehlgeschlagen)\"\n",
    "\n",
    "    raw_title_part = parts[1]\n",
    "\n",
    "    title = raw_title_part.split('\\n', 1)[0]\n",
    "\n",
    "    daten[\"Titel\"] = title \n",
    "\n",
    "    return daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fe8e8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_guests(text: str) -> List[GastEintrag]:\n",
    "    ergebnisse = []\n",
    "\n",
    "\n",
    "    keywords = [\"Studiogästen\", \"Studiogast\"]\n",
    "    \n",
    "    relevant_part = text  # Fallback: Ganzer Text, falls kein Keyword gefunden wird\n",
    "\n",
    "    for kw in keywords:\n",
    "        if kw in text:\n",
    "            # Wir splitten beim ersten gefundenen Keyword und nehmen den Teil danach\n",
    "            relevant_part = text.split(kw, 1)[1]\n",
    "            found = True\n",
    "\n",
    "    blöcke = re.finditer(r'([^)]+)\\(([^)]+)\\)', relevant_part)\n",
    "\n",
    "    for match in blöcke:\n",
    "        namen_roh = match.group(1).strip()\n",
    "        beschreibung = match.group(2).strip()\n",
    "\n",
    "        # Wir ignorieren Satzzeichen am Ende des Namensblocks (z.B. Kommas vom vorherigen Eintrag)\n",
    "        if namen_roh.startswith(\",\"):\n",
    "            namen_roh = namen_roh[1:].strip()\n",
    "\n",
    "        # Schritt C: Namen aus dem Roh-String extrahieren\n",
    "        # Muster: Wort (TitleCase) + optional Mittelname/Initial + WORT (UPPERCASE)\n",
    "        # \\b           -> Wortgrenze\n",
    "        # [A-ZÄÖÜ][a-zäöü]+ -> Vorname (Großbuchstabe gefolgt von Kleinbuchstaben)\n",
    "        # (?: ... )* -> Optionale Mittelnamen oder Initialen (z.B. \"Philip D\" oder \"Hans Peter\")\n",
    "        # \\s+          -> Leerzeichen\n",
    "        # [A-ZÄÖÜß]+   -> NACHNAME (Nur Großbuchstaben, mind. 2 Zeichen um \"I\" vs \"IN\" zu unterscheiden, oder einfach +)\n",
    "        \n",
    "        name_pattern = r'\\b[A-ZÄÖÜ][a-zäöü]+(?:\\s+(?:[A-Z]\\.?|[A-ZÄÖÜ][a-zäöü]+))*\\s+[A-ZÄÖÜß]+(?:-[A-ZÄÖÜß]+)?'\n",
    "        \n",
    "        gefundenen_namen = re.findall(name_pattern, namen_roh)\n",
    "\n",
    "        if gefundenen_namen:\n",
    "            eintrag = GastEintrag(\n",
    "                namen=gefundenen_namen,\n",
    "                beschreibung=beschreibung\n",
    "            )\n",
    "            ergebnisse.append(eintrag)\n",
    "\n",
    "    return ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "fb1877fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_persons(episodes):\n",
    "    \"\"\"\n",
    "    Extrahiert Gäste und gruppiert sie pro Person.\n",
    "    Ausgabe: Eine Zeile pro Gast.\n",
    "    - Alle Beschreibungen werden gesammelt und kommagetrennt.\n",
    "    - Alle Auftritte werden als eigene Spalten angefügt.\n",
    "    \"\"\"\n",
    "\n",
    "    # Struktur: \n",
    "    # { \n",
    "    #   \"Name\": { \n",
    "    #       \"descriptions\": {\"Schauspieler\", \"Autor\"},  <-- Jetzt ein Set statt String\n",
    "    #       \"appearances\": [(\"Datum_Sort\", \"String_Info\"), ...] \n",
    "    #   } \n",
    "    # }\n",
    "    guest_map = {}\n",
    "\n",
    "    for episode_text in episodes:\n",
    "        if not episode_text:\n",
    "            continue\n",
    "\n",
    "        # 1. Daten extrahieren\n",
    "        daten = extrahiere_sendungs_details(episode_text)\n",
    "\n",
    "        if \"Sachinhalt\" not in episode_text:\n",
    "            continue\n",
    "\n",
    "        text_flat = episode_text.replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "        try:\n",
    "            content_part = text_flat.split(\"Sachinhalt\", 1)[1]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        # Keywords zum Abschneiden\n",
    "        keywords = [';', 'über', 'Themen', 'Thema', 'Themen:', 'Thema:', 'Jugendeignung', 'Schwerpunktthemen:', 'Schwerpunktthemen', 'Scherpunktthemen:']\n",
    "        keywords.sort(key=len, reverse=True)\n",
    "        pattern_split = \"|\".join(map(re.escape, keywords))\n",
    "        \n",
    "        guests_section = re.split(pattern_split, content_part, maxsplit=1)[0]\n",
    "\n",
    "        # Regex ausführen\n",
    "        gefundene_eintraege = extract_guests(guests_section)\n",
    "        \n",
    "        # Auftritt-String formatieren\n",
    "        auftritt_string = f\"{daten['Titel']} (Folge {daten['Folge']}, {daten['Datum']})\"\n",
    "        \n",
    "        # Datum parsen\n",
    "        try:\n",
    "            datum_sort = pd.to_datetime(daten[\"Datum\"], format=\"%d.%m.%Y\", errors=\"coerce\")\n",
    "        except:\n",
    "            datum_sort = pd.NaT\n",
    "\n",
    "        for eintrag in gefundene_eintraege:\n",
    "            for einzelner_name in eintrag.namen:\n",
    "                name_clean = einzelner_name.strip()\n",
    "                \n",
    "                # Wenn Gast noch nicht bekannt, neu anlegen\n",
    "                if name_clean not in guest_map:\n",
    "                    guest_map[name_clean] = {\n",
    "                        \"descriptions\": set(), # Set für eindeutige Beschreibungen\n",
    "                        \"appearances\": [] \n",
    "                    }\n",
    "                \n",
    "                # Beschreibung hinzufügen (wenn vorhanden)\n",
    "                if eintrag.beschreibung:\n",
    "                    clean_desc = eintrag.beschreibung.strip()\n",
    "                    if clean_desc: # Nur nicht-leere Beschreibungen\n",
    "                        guest_map[name_clean][\"descriptions\"].add(clean_desc)\n",
    "\n",
    "                # Auftritt hinzufügen\n",
    "                guest_map[name_clean][\"appearances\"].append((datum_sort, auftritt_string))\n",
    "\n",
    "    # --- DATAFRAME ERSTELLEN ---\n",
    "    \n",
    "    final_rows = []\n",
    "    \n",
    "    for name, info in guest_map.items():\n",
    "        # 1. Auftritte sortieren\n",
    "        sorted_apps = sorted(info[\"appearances\"], key=lambda x: (x[0] is pd.NaT, x[0]))\n",
    "        app_strings = [app[1] for app in sorted_apps]\n",
    "        \n",
    "        # 2. Beschreibungen zusammenfügen\n",
    "        # Wir sortieren die Liste, damit die Reihenfolge deterministisch ist\n",
    "        # und joinen sie mit Komma und Leerzeichen\n",
    "        combined_description = \", \".join(sorted(list(info[\"descriptions\"])))\n",
    "        \n",
    "        row = {\n",
    "            \"Name\": name,\n",
    "            \"Beschreibung\": combined_description, # Alle Beschreibungen hier\n",
    "            \"Anzahl_Auftritte\": len(app_strings)\n",
    "        }\n",
    "        \n",
    "        # Dynamisch Spalten \"Public Appearance X\" füllen\n",
    "        for idx, app_str in enumerate(app_strings, start=1):\n",
    "            row[f\"Public Appearance {idx}\"] = app_str\n",
    "            \n",
    "        final_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(final_rows)\n",
    "\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=\"Name\")\n",
    "        \n",
    "        # Spalten ordnen\n",
    "        cols = list(df.columns)\n",
    "        base_cols = [\"Name\", \"Beschreibung\", \"Anzahl_Auftritte\"]\n",
    "        appearance_cols = [c for c in cols if c.startswith(\"Public Appearance\")]\n",
    "        \n",
    "        # Appearance Spalten numerisch korrekt sortieren (1, 2, ... 10)\n",
    "        appearance_cols.sort(key=lambda x: int(x.split(\" \")[-1]))\n",
    "        \n",
    "        # Zusammenfügen, dabei sicherstellen, dass nur existierende Spalten genutzt werden\n",
    "        final_cols = base_cols + appearance_cols\n",
    "        # Falls durch das DataFrame erstellen noch andere Spalten entstanden wären (unwahrscheinlich), fliegen sie hier raus\n",
    "        df = df[final_cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1366aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_episodes(episodes):\n",
    "    \"\"\"\n",
    "    Erstellt einen DataFrame mit einer Zeile pro Episode.\n",
    "    Enthält Metadaten und dynamische Spalten für jeden Gast (Gast_1, Gast_2...).\n",
    "    Keine Beschreibungen.\n",
    "    \"\"\"\n",
    "\n",
    "    alle_daten = []\n",
    "\n",
    "    for episode_text in episodes:\n",
    "        # Sicherheitscheck: Ist überhaupt Text da?\n",
    "        if not episode_text or not episode_text.strip():\n",
    "            continue\n",
    "\n",
    "        # 1. Daten extrahieren \n",
    "        # (Wir nutzen hier deine existierende Helper-Funktion)\n",
    "        daten = extrahiere_sendungs_details(episode_text)\n",
    "\n",
    "        # 2. Prüfen, ob \"Sachinhalt\" vorhanden ist (dort stehen die Gäste)\n",
    "        if \"Sachinhalt\" not in episode_text:\n",
    "            continue\n",
    "\n",
    "        text_flat = episode_text.replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "        try:\n",
    "            # Wir nehmen alles NACH dem ersten \"Sachinhalt\"\n",
    "            content_part = text_flat.split(\"Sachinhalt\", 1)[1]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        # 3. Alles nach den Themen-Keywords abschneiden\n",
    "        keywords = [';', 'über', 'Themen', 'Thema', 'Themen:', 'Thema:', 'Jugendeignung', 'Schwerpunktthemen:', 'Schwerpunktthemen', 'Scherpunktthemen:']\n",
    "        keywords.sort(key=len, reverse=True)\n",
    "        \n",
    "        pattern_split = \"|\".join(map(re.escape, keywords))\n",
    "        # Split und nimm nur den ersten Teil (die Gästeliste)\n",
    "        guests_section = re.split(pattern_split, content_part, maxsplit=1)[0]\n",
    "\n",
    "        # 4. Gäste extrahieren (Nutzung deiner existierenden Funktion)\n",
    "        gefundene_eintraege = extract_guests(guests_section)\n",
    "        \n",
    "        # 5. Basis-Dictionary für die Zeile erstellen\n",
    "        # Hier verwende ich die Keys, die du in 'extract_episode_csv' wolltest\n",
    "        zeilen_daten = {\n",
    "            \"Sendungstitel\": daten.get(\"Titel\"),\n",
    "            \"Staffel\": daten.get(\"Staffel\"),\n",
    "            \"Folge\": daten.get(\"Folge\"),\n",
    "            \"FolgenNr\": daten.get(\"FolgenNr\"),\n",
    "            \"Publikationsdatum\": daten.get(\"Datum\"), # Umbenannt zu Publikationsdatum für Konsistenz\n",
    "            # Optional: Sendezeit/Dauer behalten, falls gewünscht, sonst löschen:\n",
    "            # \"Dauer\": daten.get(\"Dauer\"),\n",
    "        }\n",
    "        \n",
    "        # 6. Gäste dynamisch als Spalten hinzufügen (OHNE Beschreibung)\n",
    "        gast_counter = 1\n",
    "\n",
    "        for eintrag in gefundene_eintraege:\n",
    "            # eintrag.namen ist eine Liste (z.B. [\"Max MÜLLER\", \"Susi MEIER\"])\n",
    "            \n",
    "            for einzelner_name in eintrag.namen:\n",
    "                key = f\"Gast_{gast_counter}\"\n",
    "                zeilen_daten[key] = einzelner_name\n",
    "                gast_counter += 1\n",
    "\n",
    "        alle_daten.append(zeilen_daten)\n",
    "\n",
    "    # --- DataFrame erstellen ---\n",
    "    df = pd.DataFrame(alle_daten)\n",
    "\n",
    "    # Sortierung logisch nach Datum\n",
    "    if not df.empty and \"Publikationsdatum\" in df.columns:\n",
    "        df[\"Datum_Sortierbar\"] = pd.to_datetime(df[\"Publikationsdatum\"], format=\"%d.%m.%Y\", errors=\"coerce\")\n",
    "        df = df.sort_values(by=\"Datum_Sortierbar\", ascending=True)\n",
    "        df = df.drop(columns=[\"Datum_Sortierbar\"])\n",
    "        \n",
    "        # Spalten sortieren: Metadaten zuerst, dann Gast_1, Gast_2 ...\n",
    "        cols = list(df.columns)\n",
    "        meta_cols = [\"Sendungstitel\", \"Staffel\", \"Folge\", \"FolgenNr\", \"Publikationsdatum\"]\n",
    "        # Alle Gast-Spalten finden\n",
    "        gast_cols = [c for c in cols if c.startswith(\"Gast_\")]\n",
    "        # Gast-Spalten numerisch sortieren (damit Gast_2 vor Gast_10 kommt)\n",
    "        gast_cols.sort(key=lambda x: int(x.split(\"_\")[1]))\n",
    "        \n",
    "        # Andere Spalten (falls vorhanden, z.B. Dauer)\n",
    "        rest_cols = [c for c in cols if c not in meta_cols and c not in gast_cols]\n",
    "        \n",
    "        final_order = meta_cols + rest_cols + gast_cols\n",
    "        # Nur Spalten nutzen, die wirklich im DF sind\n",
    "        final_order = [c for c in final_order if c in df.columns]\n",
    "        \n",
    "        df = df[final_order]\n",
    "\n",
    "\n",
    "    df[\"instance_of\"] = 'Q21191270'\n",
    "    df[\"part_of_series\"] = 'Q1499182'\n",
    "    df[\"genre\"] = 'Q622812'\n",
    "    df[\"presenter\"] = 'Q43773'\n",
    "    df[\"original_broadcaster\"] = 'Q48989'\n",
    "    df[\"country_of_origin\"] = 'Q183'\n",
    "    df[\"original_language_of_film_or_TV_show\"] = 'Q188'\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_seasons(episodes):\n",
    "    \"\"\"\n",
    "    Erstellt eine Übersicht der Staffeln basierend auf Jahren.\n",
    "    Staffel 1 beginnt 2008.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Zwischenspeicher: Key = Jahr, Value = Liste von Datum-Objekten\n",
    "    # Beispiel: { 2018: [datetime(2018,1,5), datetime(2018,1,6)...] }\n",
    "    years_data = {}\n",
    "\n",
    "    # Regex für das strikte Titelformat: \"Markus Lanz DD.MM.YYYY\"\n",
    "    # Sucht nach \"Markus Lanz \" gefolgt von Tag.Monat.Jahr\n",
    "    regex_title_strict = re.compile(r\"Markus Lanz \\s*\\d{2}\\.\\d{2}\\.\\d{4}\")\n",
    "    \n",
    "    # Regex um das Datum generell zu finden (dd.mm.yyyy)\n",
    "    regex_date = re.compile(r\"(\\d{2}\\.\\d{2}\\.\\d{4})\")\n",
    "\n",
    "    for episode_text in episodes:\n",
    "        if not episode_text:\n",
    "            continue\n",
    "            \n",
    "        # 1. Relevante Zeilen finden\n",
    "        # Wir suchen die Zeile, die mit \"Sendetitel\" beginnt, um das Format zu prüfen\n",
    "        lines = episode_text.splitlines()\n",
    "        title_line = \"\"\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"Sendetitel\"):\n",
    "                title_line = line.strip()\n",
    "                break\n",
    "        \n",
    "        # Falls kein Sendetitel gefunden wurde, suchen wir im ganzen Text nach dem Datum\n",
    "        # (Fallback, falls Formatierung abweicht)\n",
    "        search_text = title_line if title_line else episode_text\n",
    "\n",
    "        # 2. Prüfen, ob Episode berücksichtigt werden soll\n",
    "        # Bedingung A: Titel hat Format \"Markus Lanz DD.MM.YYYY\"\n",
    "        condition_a = bool(regex_title_strict.search(title_line))\n",
    "        \n",
    "        # Bedingung B: Es gibt einen \"Staffel\" Eintrag im Text\n",
    "        condition_b = \"Staffel\" in episode_text\n",
    "        \n",
    "        if not (condition_a or condition_b):\n",
    "            continue\n",
    "\n",
    "        # 3. Datum extrahieren\n",
    "        # Wir bevorzugen das Datum aus dem Titel, sonst das erste Datum im Text\n",
    "        date_match = regex_date.search(title_line)\n",
    "        if not date_match:\n",
    "            # Fallback: Suche erstes Datum im gesamten Block (z.B. bei Info Sendedatum)\n",
    "            date_match = regex_date.search(episode_text)\n",
    "            \n",
    "        if date_match:\n",
    "            date_str = date_match.group(1)\n",
    "            try:\n",
    "                dt_obj = pd.to_datetime(date_str, format=\"%d.%m.%Y\")\n",
    "                year = dt_obj.year\n",
    "                \n",
    "                # Nur Jahre ab 2008 berücksichtigen (gemäß Logik Staffel 1 = 2008)\n",
    "                if year >= 2008:\n",
    "                    if year not in years_data:\n",
    "                        years_data[year] = []\n",
    "                    years_data[year].append(dt_obj)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # 4. DataFrame aus den gesammelten Daten bauen\n",
    "    season_rows = []\n",
    "    \n",
    "    # Sortierte Jahre durchgehen\n",
    "    for year in sorted(years_data.keys()):\n",
    "        dates = years_data[year]\n",
    "        \n",
    "        # Metriken berechnen\n",
    "        start_date = min(dates)\n",
    "        end_date = max(dates)\n",
    "        count = len(dates)\n",
    "        \n",
    "        # Staffelnummer berechnen (2008 -> 1, 2009 -> 2, etc.)\n",
    "        season_num = year - 2007\n",
    "        season_name = f\"Markus Lanz, Staffel {season_num}\"\n",
    "        \n",
    "        season_rows.append({\n",
    "            \"Staffel Name\": season_name,\n",
    "            \"start_time\": start_date, # Als Timestamp lassen oder formatieren\n",
    "            \"end_time\": end_date,\n",
    "            \"Anzahl an Folgen\": count\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(season_rows)\n",
    "    \n",
    "    # Formatierung der Datumsspalten als String (DD.MM.YYYY) für schönere Ausgabe\n",
    "    if not df.empty:\n",
    "        df[\"start_time\"] = df[\"start_time\"].dt.strftime(\"%d.%m.%Y\")\n",
    "        df[\"end_time\"] = df[\"end_time\"].dt.strftime(\"%d.%m.%Y\")\n",
    "\n",
    "\n",
    "    df[\"instance_of\"] = 'Q3464665'\n",
    "    df[\"part_of_series\"] = 'Q1499182'\n",
    "    df[\"genre\"] = 'Q622812'\n",
    "    df[\"presenter\"] = 'Q43773'\n",
    "    df[\"original_broadcaster\"] = 'Q48989'\n",
    "    df[\"country_of_origin\"] = 'Q183'\n",
    "    df[\"original_language_of_film_or_TV_show\"] = 'Q188'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca939b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Extraktion von Markus Lanz_2008-2010.pdf...\n",
      "179 Folgen in Markus Lanz_2008-2010.pdf gefunden\n",
      "Starte Extraktion von Markus Lanz_2011-2015.pdf...\n",
      "638 Folgen in Markus Lanz_2011-2015.pdf gefunden\n",
      "Starte Extraktion von Markus Lanz_2016-2020.pdf...\n",
      "666 Folgen in Markus Lanz_2016-2020.pdf gefunden\n",
      "Starte Extraktion von Markus Lanz_2021-2024.pdf...\n",
      "555 Folgen in Markus Lanz_2021-2024.pdf gefunden\n",
      "2038\n",
      "Found Duplicate: Markus Lanz 12.11.2015\n",
      "Found Duplicate: Markus Lanz 15.12.2015\n"
     ]
    }
   ],
   "source": [
    "pdf_names = ['Markus Lanz_2008-2010.pdf', 'Markus Lanz_2011-2015.pdf', 'Markus Lanz_2016-2020.pdf', 'Markus Lanz_2021-2024.pdf']\n",
    "\n",
    "episode_strings = []\n",
    "\n",
    "for pdf_name in pdf_names:\n",
    "    path = f\"ZDF_Archiv/{pdf_name}\"\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            print(f\"Starte Extraktion von {pdf_name}...\")\n",
    "            episodes_from_pdf = build_text_list_from_pdf(path)\n",
    "            #text_files(episodes, filename=f\"textfiles/{pdf_name}_episodes.txt\")\n",
    "            for episode in reversed(episodes_from_pdf): \n",
    "                episode_strings.append(episode)\n",
    "            print(f\"{len(episodes_from_pdf)} Folgen in {pdf_name} gefunden\")\n",
    "        except Exception as e:\n",
    "            print(f\"Kritischer Fehler: {e}\")\n",
    "    else:\n",
    "        print(f\"Datei '{path}' nicht im Ordner gefunden.\")\n",
    "\n",
    "\n",
    "episode_strings = clean_episodes(episode_strings)\n",
    "\n",
    "print(f\"Episodes found: {len(episode_strings)}\")\n",
    "\n",
    "\n",
    "persondf = extract_persons(episode_strings)\n",
    "persondf.to_csv(f\"person/Personen.csv\", index=False)\n",
    "\n",
    "episodesdf = extract_episodes(episode_strings)\n",
    "episodesdf.to_csv(f\"episodes/Episoden.csv\", index=False)\n",
    "\n",
    "seasonsdf = extract_seasons(episode_strings)\n",
    "seasonsdf.to_csv(f\"seasons/Staffeln.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0705963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
